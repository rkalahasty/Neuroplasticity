#%%-------------------------------------------------------------------
# Clears all the variables from the workspace of the application
# might not work unless this cell is explicitly run
def clear_all():
    gl = globals().copy()
    for var in gl:
        if var[0] == '_': continue
        if 'func' in str(globals()[var]): continue
        if 'module' in str(globals()[var]): continue

        del globals()[var]


if __name__ == "__main__":
    clear_all()
#%%-----------------------------------------------------------------------------

import os
print(os.getcwd())
dir = r'C:\Users\17033\BiologicalSF\HL\saving'# use r'path with spaces' if there are spaces in the path name
os.chdir(dir)# print(f'current working direction is {os.getcwd()}')
print(os.getcwd())


import numpy as np
import torch
import matplotlib.pyplot as plt
import matplotlib.cm as cm
from HLmodel import CTRNN, CTRNN_HL # from file import function
from generateINandTARGETOUT import generateINandTARGETOUT  # from file import function
torch.autograd.set_detect_anomaly(True)
from tqdm import tqdm
from resultutils import smallWorldIndex

randseed = 123
np.random.seed(randseed);
torch.manual_seed(randseed)  # set random seed for reproducible results

##############################################################################
# %% initialize network
dim_input = 2  # number of inputs, the first input dimension is for the values to be remembered, the second input dimension is for the go-cue
dim_recurrent = 100  # number of recurrent units
dim_output = 4  # number of outputs, the first output dimension is for the value inputted first, the second output dimension is for the value inputted second
numT = 150  # number of timesteps in a trial
numtrials = 50  # number of trials used for each parameter update, i.e. the number of trials generated by the function generateINandTARGETOUT for each minibatch
nonlinearity = 'retanh'
numparameterupdates = 10000  # number of parameter updates
pset_saveparameters = np.unique(np.concatenate((np.arange(0, 6), np.array([50, 100, 150, 200]), np.round(
    np.linspace(0, numparameterupdates, num=20, endpoint=True))))).astype(
    int)  # save parameters when parameter update p is a member of pset_saveparameters, save as int so we can use elements to load model parameters: for example, model_parameterupdate211.pth versus model_parameterupdate211.0.pth

# model = ElmanRNN(dim_input, dim_recurrent, dim_output, LEARN_h0=False); modelname = 'Elman RNN'
# ------------------------------------------------------
# constants that are not learned: dt, Tau, bhneverlearn
dt = 1
Tau = 10 * torch.ones(dim_recurrent)
NOISEAMPLITUDE = 0.1  # 0.1, standard deviation of firing rate noise, bhneverlearn = NOISEAMPLITUDE*torch.randn(numtrials, numT, dim_recurrent)
bhneverlearn = NOISEAMPLITUDE * torch.randn(numtrials, numT, dim_recurrent)  # (numtrials, numT, dim_recurrent) tensor

# parameters to be learned: Wahh, Wahx, Wyh, bah, by, ah0(optional)
ah0 = torch.zeros(dim_recurrent)
bah = torch.zeros(dim_recurrent)
by = torch.zeros(dim_output)

# Saxe at al. 2014 "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks"
# We empirically show that if we choose the initial weights in each layer to be a random orthogonal matrix (satisifying W'*W = I), instead of a scaled random Gaussian matrix, then this orthogonal random initialization condition yields depth independent learning times just like greedy layerwise pre-training.
# [u,s,v] = svd(A); A = u*s*v’; columns of u are eigenvectors of covariance matrix A*A’; rows of v’ are eigenvectors of covariance matrix A’*A; s is a diagonal matrix that has elements = sqrt(eigenvalues of A’*A and A*A’)
Wahh = np.random.randn(dim_recurrent, dim_recurrent)
u, s, vT = np.linalg.svd(Wahh)  # np.linalg.svd returns v transpose!
Wahh_check = u @ np.diag(s) @ vT
C1 = Wahh;
C2 = Wahh_check
print(
    f"Do C1 and C2 have the same shape and are element-wise equal within a tolerance? {C1.shape == C2.shape and np.allclose(C1, C2)}")
Wahh = u @ np.diag(1.0 * np.ones(dim_recurrent)) @ vT  # make the eigenvalues large so they decay slowly
Wahh = torch.tensor(Wahh, dtype=torch.float32);
initname = '_initWahhsaxe'

# Sussillo et al. 2015 "A neural network that finds a naturalistic solution for the production of muscle activity"
Wahx = torch.randn(dim_recurrent, dim_input) / np.sqrt(dim_input)
Wahh = 1.5 * torch.randn(dim_recurrent, dim_recurrent) / np.sqrt(dim_recurrent)
A = .5 * torch.randn(dim_recurrent, dim_recurrent) / np.sqrt(dim_recurrent)
B = .5 * torch.randn(dim_recurrent, dim_recurrent) / np.sqrt(dim_recurrent)
C = .5 * torch.randn(dim_recurrent, dim_recurrent) / np.sqrt(dim_recurrent)
D = .5 * torch.randn(dim_recurrent, dim_recurrent) / np.sqrt(dim_recurrent)
lrs = .01 * torch.randn(dim_recurrent, dim_recurrent) / np.sqrt(dim_recurrent)

initname = '_initWahhsussillo'
Wyh = torch.zeros(dim_output, dim_recurrent)


#CHANGE WHAT MODEL YOU ARE USING HERE
model1 = CTRNN(dim_input, dim_recurrent, dim_output, Wahx=Wahx, Wahh=Wahh, Wyh=Wyh, bah=bah, by=by,
                 nonlinearity=nonlinearity, ah0=ah0, LEARN_ah0=False)
modelname1 = 'CTRNN'

model2 = CTRNN_HL(dim_input, dim_recurrent, dim_output, Wahx=Wahx.clone(), Wahh0=Wahh.clone(), Wyh=Wyh.clone(), bah=bah.clone(), by=by.clone(),
                 nonlinearity=nonlinearity, ah0=ah0.clone(), LEARN_ah0=False, A=A, B=B, C=C, D=D, lrs=lrs)
modelname2 = 'HL+CTRNN'
figuresuffix = ''

##############################################################################
#                        Small World Post Adapt
##############################################################################

import os
modelname = "HL+CTRNN"
numparameters = 60704 if "HL" in modelname else 10704
figdir = fr'{modelname}_{nonlinearity}_dimrecurrent{dim_recurrent}_{numparameters}parameters_{10000}parameterupdates{initname}_NOISEAMPLITUDE{NOISEAMPLITUDE}_rng{randseed}'
print(os.curdir)
if not os.path.exists(figdir):
    os.makedirs(figdir)
model = torch.load(figdir + f'/Final{modelname}.pth')
np.random.seed(1234);
torch.manual_seed(1234)  # set random seed for reproducible results
numTtest = numT
numtrialstest = 1
IN, TARGETOUT, itimeRNN = generateINandTARGETOUT(dim_input, dim_output, numTtest, numtrialstest)
TARGETOUT = TARGETOUT.numpy()
bhneverlearn = 0 * torch.randn(numtrialstest, numTtest, dim_recurrent)  # (numtrials, numT, dim_recurrent) tensor
output, h, Wahh= model(IN, dt, Tau, bhneverlearn)
output = output.detach().numpy()
h = h.detach().numpy()
Wahh = Wahh.detach().numpy()
print(Wahh.shape)
mWahh = np.mean(Wahh, axis=0)
print(smallWorldIndex(mWahh))

inhibitoryCount = 0
for i in range(len(mWahh)):
    for j in range(len(mWahh)):
        inhibitoryCount += 1 if (mWahh[i][j] < 0) else 0

print(inhibitoryCount)

import os
modelname = "CTRNN"
numparameters = 60704 if "HL" in modelname else 10704
figdir = fr'{modelname}_{nonlinearity}_dimrecurrent{dim_recurrent}_{numparameters}parameters_{10000}parameterupdates{initname}_NOISEAMPLITUDE{NOISEAMPLITUDE}_rng{randseed}'
if not os.path.exists(figdir):
    os.makedirs(figdir)
model = torch.load(figdir + f'/Final{modelname}.pth')
Wahh = model.fc_h2ah.weight.detach().numpy()
print(smallWorldIndex(Wahh))


inhibitoryCount = 0
for i in range(len(Wahh)):
    for j in range(len(Wahh)):
        inhibitoryCount += 1 if Wahh[i][j] < 0 else 0

print(inhibitoryCount)

import os
modelname = "HL+CTRNN"
numparameters = 60704 if "HL" in modelname else 10704
figdir = fr'{modelname}_{nonlinearity}_dimrecurrent{dim_recurrent}_{numparameters}parameters_{10000}parameterupdates{initname}_NOISEAMPLITUDE{NOISEAMPLITUDE}_rng{randseed}'
if not os.path.exists(figdir):
    os.makedirs(figdir)
model = torch.load(figdir + f'/Final{modelname}.pth')
Wahh = model.Wahh0.detach().numpy()
print(smallWorldIndex(Wahh))




# # ##############################################################################
# #                            Test data
# ##############################################################################
# for modelname in ['CTRNN', 'HL+CTRNN']:
#     # ---------------make folder to store files---------------
#     import os
#     numparameters = 60704 if "HL" in modelname else 10704
#     figdir = fr'{modelname}_{nonlinearity}_dimrecurrent{dim_recurrent}_{numparameters}parameters_{10000}parameterupdates{initname}_NOISEAMPLITUDE{NOISEAMPLITUDE}_rng{randseed}'
#     print(os.curdir)
#     if not os.path.exists(figdir):
#         os.makedirs(figdir)
#     model = torch.load(figdir + f'/Final{modelname}.pth')
#
#     numparameters = sum(p.numel() for p in model.parameters() if
#                         p.requires_grad)  # model.parameters include those defined in __init__ even if they are not used in forward pass
#
#     np.random.seed(1234);
#     torch.manual_seed(1234)  # set random seed for reproducible results
#     numTtest = numT
#     numtrialstest = 1000
#     IN, TARGETOUT, itimeRNN = generateINandTARGETOUT(dim_input, dim_output, numTtest, numtrialstest)
#     TARGETOUT = TARGETOUT.numpy()
#     bhneverlearn = 0 * torch.randn(numtrialstest, numTtest, dim_recurrent)  # (numtrials, numT, dim_recurrent) tensor
#     output, h, Wahh = model(IN, dt, Tau, bhneverlearn)
#     output = output.detach().numpy()
#     h = h.detach().numpy()
#     Wahh = Wahh.detach().numpy()
#     # IN:        (numtrialstest, numTtest, dim_input) tensor
#     # TARGETOUT: (numtrialstest, numTtest, dim_output) tensor
#     # h:         (numtrialstest, numTtest, dim_recurrent) tensor
#
#
#     ##############################################################################
#     # %% percent normalized test error as a function of training iteration
#     pset = pset_saveparameters[pset_saveparameters >= 5]
#     pset = pset[:-1]
#     # pset = pset_saveparameters[pset_saveparameters>=0]
#     normalizederror_store = -700 * np.ones((pset.shape[0]))
#     for ip, p in tqdm(enumerate(pset)):
#         # load models, first re-create the model structure and then load the state dictionary into it
#         # model = LSTM(dim_input, dim_recurrent, dim_output, LEARN_c0h0=True, nonlinearity='retanh', noiseamplitude=0); modelname = 'LSTM'
#         # model.load_state_dict(torch.load(figdir + 'model.pth'))
#         checkpoint = torch.load(figdir + f'/model_parameterupdate{p}.pth');
#         model.load_state_dict(checkpoint['model_state_dict']);
#
#         output, h, hold = model(IN, dt, Tau, bhneverlearn)
#         output = output.detach().numpy()
#         h = h.detach().numpy()
#         # output: (numtrialstest, numTtest, dim_output) tensor
#         # h:      (numtrialstest, numTtest, dim_recurrent) tensor
#
#         # normalized error, if RNN output is constant for each dim_output (each dim_output can be a different constant) then normalizederror = 100%
#         # outputforerror = output(itimeRNN==1)
#         # TARGETOUTforerror = TARGETOUT(itimeRNN==1)
#         # normalizederror = 100*((outputforerror(:) - TARGETOUTforerror(:))' @ (outputforerror(:) - TARGETOUTforerror(:))) / ((mean(TARGETOUTforerror(:)) - TARGETOUTforerror(:))'@(mean(TARGETOUTforerror(:)) - TARGETOUTforerror(:)));% normalized error when using outputs for which itimeRNNtest = 1
#         normalizederror = 0;
#         for i in range(dim_output):
#             outputforerror = output[:, :, i]  # (numtrialstest, numTtest)
#             outputforerror = outputforerror[itimeRNN[:, :, i] == 1][:, None]  # (something,1)
#             TARGETOUTforerror = TARGETOUT[:, :, i]  # (numtrialstest, numTtest)
#             TARGETOUTforerror = TARGETOUTforerror[itimeRNN[:, :, i] == 1][:, None]  # (something,1)
#             A = outputforerror - TARGETOUTforerror
#             B = np.mean(TARGETOUTforerror) - TARGETOUTforerror
#             normalizederror = normalizederror + 100 * (A.transpose() @ A) / (
#                         B.transpose() @ B)  # normalized error when using outputs for which itimeRNN = 1
#             # B = torch.mean(TARGETOUTforerror) - TARGETOUTforerror
#             # normalizederror = normalizederror + 100*(A.transpose(0,1) @ A) / (B.transpose(0,1) @ B)# normalized error when using outputs for which itimeRNN = 1
#         # normalizederror = torch.squeeze(normalizederror) / dim_output# use torch.squeeze so normalized error goes from having shape (1,1) to being a number, this makes it easier to plot in figure titles, etc.
#         normalizederror = np.squeeze(
#             normalizederror) / dim_output  # use np.squeeze so normalized error goes from having shape (1,1) to being a number, this makes it easier to plot in figure titles, etc.
#         normalizederror_store[ip] = normalizederror
#     np.save(f'{figdir}/pset.npy', pset)
#     np.save(f'{figdir}/normalizederror_store.npy', normalizederror_store)
#     # pset = np.load('pset.npy')
#     # normalizederror_store = np.load('normalizederror_store.npy')
#
#     fig, ax = plt.subplots()  # normalized error versus number of parameter updates
#     fontsize = 14
#     handle = ax.plot(pset, normalizederror_store, 'k-', linewidth=3)
#     # ax.legend(handles=handle, labels=[f'{modelname} {normalizederror_store[0,-1]:.6g}%', f'{model2name} {normalizederror_store[1,-1]:.6g}%', f'{model3name} {normalizederror_store[2,-1]:.6g}%', f'{model4name} {normalizederror_store[3,-1]:.6g}%', f'{model5name} {normalizederror_store[4,-1]:.6g}%'], loc='best', frameon=True)
#     ax.legend(handles=handle, labels=[f'{modelname} {normalizederror_store[-1]:.6g}%'], loc='best', frameon=True)
#     ax.set_xlabel('Number of parameter updates', fontsize=fontsize)
#     ax.set_ylabel('Normalized error', fontsize=fontsize)
#     imin = np.argmin(normalizederror_store)  # index of minimum normalized error
#
#     if imin == (
#             pset.size - 1):  # if the minimum normalized error occurs after the last parameter update only put the error after parameter updates pset[0] and pset[-1] in the title, remember pset[pset.size-1] gives the last element of pset
#         ax.set_title(
#             f'{numtrialstest} test trials, {numTtest} timesteps, {numparameterupdates} parameter updates\nError after {pset[0]} parameter updates = {normalizederror_store[0]:.6g}%\nError after {pset[imin]} parameter updates = {normalizederror_store[imin]:.6g}%',
#             fontsize=fontsize)
#     else:
#         ax.set_title(
#             f'{numtrialstest} test trials, {numTtest} timesteps, {numparameterupdates} parameter updates\nError after {pset[0]} parameter updates = {normalizederror_store[0]:.6g}%\nError after {pset[imin]} parameter updates = {normalizederror_store[imin]:.6g}%\nError after {pset[-1]} parameter updates = {normalizederror_store[-1]:.6g}%',
#             fontsize=fontsize)
#     ax.tick_params(axis='both', labelsize=fontsize)
#     fig.savefig('%s/errornormalized_test_numT%g%s.pdf' % (figdir, numTtest, figuresuffix),
#                 bbox_inches='tight')  # add bbox_inches='tight' to keep title from being cutoff
#
#     ##############################################################################
#     # %% load model with lowest test error
#
#     # torch.save(model, figdir + 'model.pth')# save the trained model’s learned parameters
#     # torch.save({'model_state_dict':model.state_dict(), 'figuresuffix':figuresuffix, 'lambdahL2':lambdahL2}, figdir + f'model_parameterupdate{numparameterupdates-1}.pth')# save the trained model’s learned parameters
#
#
#     # load models, first re-create the model structure and then load the state dictionary into it
#     numparameterupdatesmodel = numparameterupdates
#     numparameterupdatesmodel = pset[np.argmin(normalizederror_store)]
#     # model = CTRNN(dim_input, dim_recurrent, dim_output, LEARN_c0h0=True, nonlinearity='retanh', noiseamplitude=0); modelname = 'LSTM'
#     # model.load_state_dict(torch.load(figdir + 'model.pth'))
#     checkpoint = torch.load(figdir + f'/model_parameterupdate{9474}.pth');
#     model.load_state_dict(checkpoint['model_state_dict']);
#     figuresuffix = checkpoint['figuresuffix']
#     # print("Model's state_dict:")
#     # for param_tensor in model.state_dict():
#     #    print(param_tensor, "\t", model.state_dict()[param_tensor].size())
#
#     ##############################################################################
#     # %% compute loss on test set
#     output, h, Wahh = model(IN, dt, Tau, bhneverlearn)
#     output = output.detach().numpy();
#     h = h.detach().numpy()
#
#     # normalized error, if RNN output is constant for each dim_output (each dim_output can be a different constant) then normalizederror = 100%
#     # outputforerror = output(itimeRNN==1)
#     # TARGETOUTforerror = TARGETOUT(itimeRNN==1)
#     # normalizederror = 100*((outputforerror(:) - TARGETOUTforerror(:))' @ (outputforerror(:) - TARGETOUTforerror(:))) / ((mean(TARGETOUTforerror(:)) - TARGETOUTforerror(:))'@(mean(TARGETOUTforerror(:)) - TARGETOUTforerror(:)));% normalized error when using outputs for which itimeRNNtest = 1
#     normalizederror = 0;
#     for i in range(dim_output):
#         outputforerror = output[:, :, i]  # (numtrialstest, numTtest)
#         outputforerror = outputforerror[itimeRNN[:, :, i] == 1][:, None]  # (something,1)
#         TARGETOUTforerror = TARGETOUT[:, :, i]  # (numtrialstest, numTtest)
#         TARGETOUTforerror = TARGETOUTforerror[itimeRNN[:, :, i] == 1][:, None]  # (something,1)
#         A = outputforerror - TARGETOUTforerror
#         B = np.mean(TARGETOUTforerror) - TARGETOUTforerror
#         normalizederror = normalizederror + 100 * (A.transpose() @ A) / (
#                     B.transpose() @ B)  # normalized error when using outputs for which itimeRNN = 1
#         # B = torch.mean(TARGETOUTforerror) - TARGETOUTforerror
#         # normalizederror = normalizederror + 100*(A.transpose(0,1) @ A) / (B.transpose(0,1) @ B)# normalized error when using outputs for which itimeRNN = 1
#     # normalizederror = torch.squeeze(normalizederror) / dim_output# use torch.squeeze so normalized error goes from having shape (1,1) to being a number, this makes it easier to plot in figure titles, etc.
#     normalizederror = np.squeeze(
#         normalizederror) / dim_output  # use np.squeeze so normalized error goes from having shape (1,1) to being a number, this makes it easier to plot in figure titles, etc.
#     # import sys; sys.exit()# stop script at current line
#
#
#     fontsize = 14
#     T = np.arange(0, numTtest)  # (numTtest,)
#
#     '''
#     plt.figure()# RNN input and output on test trials
#     #for itrial in range(numtrialstest):
#     for itrial in range(5):
#         plt.clf()
#         #----colormaps----
#         cool = cm.get_cmap('cool', dim_input)
#         colormap_input = cool(range(dim_input))# (dim_input, 4) array columns 1,2,3 are the RGB values, column 4 sets the transparency/alpha, datapoint[0] has color colormap[0,:]
#         #----plot single input and output for legend----
#         plt.plot(T, IN[itrial,:,0], c=colormap_input[0,:], linewidth=3, label='Input'); plt.plot(T, TARGETOUT[itrial,:,0], c=[0,0,0,1], linewidth=3, label='Output: target'); plt.plot(T, output[itrial,:,0], 'r--', linewidth=3, label='Output: RNN')# for legend
#         #----plot all inputs and outputs----
#         for i in range(dim_input):
#             plt.plot(T, IN[itrial,:,i], c=colormap_input[i,:], linewidth=3)
#         for i in range(dim_output):
#             plt.plot(T, TARGETOUT[itrial,:,i], c=[0,0,0,1], linewidth=3)# black
#             plt.plot(T, output[itrial,:,i], 'r--', linewidth=3)# red
#         plt.xlabel('Timestep', fontsize=fontsize)
#         plt.legend()
#         plt.title(f'{modelname}, trial {itrial}\n{numtrialstest} test trials, {numTtest} timesteps in simulation\n{numparameterupdatesmodel} parameter updates, normalized error = {normalizederror:.6g}%', fontsize=fontsize)
#         plt.xlim(left=0)
#         #plt.show(); input("Press Enter to continue...")# pause the program until the user presses Enter, https://stackoverflow.com/questions/21875356/saving-a-figure-after-invoking-pyplot-show-results-in-an-empty-file
#         plt.savefig('%s/testtrial%g_numTtest%g_%gparameterupdates_%s%s.pdf'%(figdir,itrial,numTtest,numparameterupdatesmodel,modelname.replace(" ", ""),figuresuffix), bbox_inches='tight')# add bbox_inches='tight' to keep title from being cutoff
#     '''
#
#     plt.figure()  # RNN input and output on test trials
#     # ----colormaps----
#     cool = cm.get_cmap('cool', dim_input)
#     colormap_input = cool(range(
#         dim_input))  # (dim_input, 4) array columns 1,2,3 are the RGB values, column 4 sets the transparency/alpha, datapoint[0] has color colormap[0,:]
#     # -----------------
#     numcurves = dim_output  # number of curves to plot
#     blacks = cm.get_cmap('Greys', numcurves + 3)
#     colormap = blacks(range(
#         numcurves + 3));  # (numcurves+3, 4) array columns 1,2,3 are the RGB values, column 4 sets the transparency/alpha
#     # colormap[0,:] = white, first row, check: plt.figure(); plt.plot(np.arange(0,10), -13*np.ones(10), c=colormap[0,:], linewidth=3)
#     # colormap[-1,:] = black, last row, check: plt.figure(); plt.plot(np.arange(0,10), -13*np.ones(10), c=colormap[-1,:], linewidth=3)
#     colormap = colormap[3:,
#                :]  # (numcurves, 4) array columns 1,2,3 are the RGB values, column 4 sets the transparency/alpha
#     colormap_outputtarget = colormap
#     # -----------------
#     numcurves = dim_output  # number of curves to plot
#     reds = cm.get_cmap('Reds', numcurves + 3)
#     colormap = reds(range(
#         numcurves + 3));  # (numcurves+3, 4) array columns 1,2,3 are the RGB values, column 4 sets the transparency/alpha
#     # colormap[0,:] = almost white, first row, check: plt.figure(); plt.plot(np.arange(0,10), -13*np.ones(10), c=colormap[0,:], linewidth=3)
#     # colormap[-1,:] = dark red, last row, check: plt.figure(); plt.plot(np.arange(0,10), -13*np.ones(10), c=colormap[-1,:], linewidth=3)
#     colormap = colormap[2:, :]  # (numcurves+1, 4) array, remove first two rows because they are too light
#     colormap = colormap[:-1, :]  # (numcurves, 4) array, remove last row because it is too similar to black
#     colormap_outputrnn = colormap
#     if numcurves == 1: colormap_outputrnn = np.array([1, 0, 0, 1])[None, :]  # (1, 4) array, red
#     # -----------------
#     # for itrial in range(numtrialstest):
#     for itrial in range(5):
#         plt.clf()
#         # ----plot single input and output for legend----
#         plt.plot(T, IN[itrial, :, 0], c=colormap_input[0, :], linewidth=3, label='Input');
#         plt.plot(T[itimeRNN[itrial, :, 0] == 1], TARGETOUT[itrial, itimeRNN[itrial, :, 0] == 1, 0], '-',
#                  c=colormap_outputtarget[-1, :], linewidth=3, label='Output: target');
#         plt.plot(T, output[itrial, :, 0], '--', c=colormap_outputrnn[-1, :], linewidth=3, label='Output: RNN')  # for legend
#         # ----plot all inputs and outputs----
#         for i in range(dim_input):
#             plt.plot(T, IN[itrial, :, i], c=colormap_input[i, :], linewidth=3)
#         for i in range(dim_output):
#             plt.plot(T[itimeRNN[itrial, :, i] == 1], TARGETOUT[itrial, itimeRNN[itrial, :, i] == 1, i],
#                      c=colormap_outputtarget[i, :], linewidth=3)  # black
#             plt.plot(T, output[itrial, :, i], '--', c=colormap_outputrnn[i, :], linewidth=3)  # red
#         plt.xlabel('Timestep', fontsize=fontsize)
#         plt.legend()
#         plt.title(
#             f'{modelname}, trial {itrial}\n{numtrialstest} test trials, {numTtest} timesteps in simulation\n{numparameterupdatesmodel} parameter updates, normalized error = {normalizederror:.6g}%',
#             fontsize=fontsize)
#         plt.xlim(left=0)
#         # plt.show(); input("Press Enter to continue...")# pause the program until the user presses Enter, https://stackoverflow.com/questions/21875356/saving-a-figure-after-invoking-pyplot-show-results-in-an-empty-file
#         plt.savefig('%s/testtrial%g_numTtest%g_%gparameterupdates_%s%s.pdf' % (
#         figdir, itrial, numTtest, numparameterupdatesmodel, modelname.replace(" ", ""), figuresuffix),
#                     bbox_inches='tight')  # add bbox_inches='tight' to keep title from being cutoff
#
#     plt.figure()  # firing of all hidden units on a single trial
#     # for itrial in range(numtrialstest):
#     for itrial in range(5):
#         plt.clf()
#         plt.plot(T, h[itrial, :, :])
#         plt.xlabel('Timestep', fontsize=fontsize)
#         plt.ylabel(f'Firing of {dim_recurrent} hidden units', fontsize=fontsize)
#         # plt.legend()
#         plt.title(
#             f'{modelname}, trial {itrial}\n{numtrialstest} test trials, {numTtest} timesteps in simulation\n{numparameterupdatesmodel} parameter updates, normalized error = {normalizederror:.6g}%',
#             fontsize=fontsize)
#         plt.xlim(left=0)
#         # plt.show(); input("Press Enter to continue...")# pause the program until the user presses Enter, https://stackoverflow.com/questions/21875356/saving-a-figure-after-invoking-pyplot-show-results-in-an-empty-file
#         plt.savefig('%s/testtrial%g_numTtest%g_%gparameterupdates_h_%s%s.pdf' % (
#         figdir, itrial, numTtest, numparameterupdatesmodel, modelname.replace(" ", ""), figuresuffix),
#                     bbox_inches='tight')  # add bbox_inches='tight' to keep title from being cutoff
#
#     plt.figure()  # firing of a single hidden unit across all trials
#     # for iunit in range(dim_recurrent):
#     for iunit in range(5):
#         plt.clf()
#         plt.plot(T, h[:, :, iunit].transpose())
#         plt.xlabel('Timestep', fontsize=fontsize)
#         plt.ylabel(f'Firing rate of unit {iunit}\nduring {numtrialstest} test trials', fontsize=fontsize)
#         # plt.legend()
#         plt.title(
#             f'{modelname}, unit {iunit}\n{numtrialstest} test trials, {numTtest} timesteps in simulation\n{numparameterupdatesmodel} parameter updates, normalized error = {normalizederror:.6g}%',
#             fontsize=fontsize)
#         plt.xlim(left=0)
#         # plt.show(); input("Press Enter to continue...")# pause the program until the user presses Enter, https://stackoverflow.com/questions/21875356/saving-a-figure-after-invoking-pyplot-show-results-in-an-empty-file
#         plt.savefig('%s/unit%g_numTtest%g_%gparameterupdates_h_%s%s.pdf' % (
#         figdir, iunit, numTtest, numparameterupdatesmodel, modelname.replace(" ", ""), figuresuffix),
#                     bbox_inches='tight')  # add bbox_inches='tight' to keep title from being cutoff
