import resultutils
from resultutils import saveTuningData, getConnectivityFigures
import numpy as np
import torch
import matplotlib.pyplot as plt
from matplotlib import cm
from HLmodel import CTRNN, CTRNN_HL # from file import function
from generateINandTARGETOUT import generateINandTARGETOUT  # from file import function
torch.autograd.set_detect_anomaly(True)

randseed = 123
np.random.seed(randseed);
torch.manual_seed(randseed)  # set random seed for reproducible results



##############################################################################
# %% initialize network
dim_input = 2  # number of inputs, the first input dimension is for the values to be remembered, the second input dimension is for the go-cue
dim_recurrent = 100  # number of recurrent units
dim_output = 4  # number of outputs, the first output dimension is for the value inputted first, the second output dimension is for the value inputted second
numT = 200  # number of timesteps in a trial
numtrials = 50  # number of trials used for each parameter update, i.e. the number of trials generated by the function generateINandTARGETOUT for each minibatch
nonlinearity = 'retanh'
numparameterupdates = 10000  # number of parameter updates
pset_saveparameters = np.unique(np.concatenate((np.arange(0, 6), np.array([50, 100, 150, 200]), np.round(
    np.linspace(0, numparameterupdates, num=20, endpoint=True))))).astype(
    int)  # save parameters when parameter update p is a member of pset_saveparameters, save as int so we can use elements to load model parameters: for example, model_parameterupdate211.pth versus model_parameterupdate211.0.pth

# model = ElmanRNN(dim_input, dim_recurrent, dim_output, LEARN_h0=False); modelname = 'Elman RNN'
# ------------------------------------------------------
# constants that are not learned: dt, Tau, bhneverlearn
dt = 1
Tau = 10 * torch.ones(dim_recurrent)
NOISEAMPLITUDE = 0.1  # 0.1, standard deviation of firing rate noise, bhneverlearn = NOISEAMPLITUDE*torch.randn(numtrials, numT, dim_recurrent)
bhneverlearn = NOISEAMPLITUDE * torch.randn(numtrials, numT, dim_recurrent)  # (numtrials, numT, dim_recurrent) tensor

# parameters to be learned: Wahh, Wahx, Wyh, bah, by, ah0(optional)
ah0 = torch.zeros(dim_recurrent)
bah = torch.zeros(dim_recurrent)
by = torch.zeros(dim_output)

# Saxe at al. 2014 "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks"
# We empirically show that if we choose the initial weights in each layer to be a random orthogonal matrix (satisifying W'*W = I), instead of a scaled random Gaussian matrix, then this orthogonal random initialization condition yields depth independent learning times just like greedy layerwise pre-training.
# [u,s,v] = svd(A); A = u*s*v’; columns of u are eigenvectors of covariance matrix A*A’; rows of v’ are eigenvectors of covariance matrix A’*A; s is a diagonal matrix that has elements = sqrt(eigenvalues of A’*A and A*A’)
Wahh = np.random.randn(dim_recurrent, dim_recurrent)
u, s, vT = np.linalg.svd(Wahh)  # np.linalg.svd returns v transpose!
Wahh_check = u @ np.diag(s) @ vT
C1 = Wahh;
C2 = Wahh_check
print(
    f"Do C1 and C2 have the same shape and are element-wise equal within a tolerance? {C1.shape == C2.shape and np.allclose(C1, C2)}")
Wahh = u @ np.diag(1.0 * np.ones(dim_recurrent)) @ vT  # make the eigenvalues large so they decay slowly
Wahh = torch.tensor(Wahh, dtype=torch.float32);
initname = '_initWahhsaxe'

# Sussillo et al. 2015 "A neural network that finds a naturalistic solution for the production of muscle activity"
Wahx = torch.randn(dim_recurrent, dim_input) / np.sqrt(dim_input)
Wahh = 1.5 * torch.randn(dim_recurrent, dim_recurrent) / np.sqrt(dim_recurrent)
A = .5 * torch.randn(dim_recurrent, dim_recurrent) / np.sqrt(dim_recurrent)
B = .5 * torch.randn(dim_recurrent, dim_recurrent) / np.sqrt(dim_recurrent)
C = .5 * torch.randn(dim_recurrent, dim_recurrent) / np.sqrt(dim_recurrent)
D = .5 * torch.randn(dim_recurrent, dim_recurrent) / np.sqrt(dim_recurrent)
lrs = .01 * torch.randn(dim_recurrent, dim_recurrent) / np.sqrt(dim_recurrent)

initname = '_initWahhsussillo'
Wyh = torch.zeros(dim_output, dim_recurrent)


#CHANGE WHAT MODEL YOU ARE USING HERE
# model = CTRNN(dim_input, dim_recurrent, dim_output, Wahx=Wahx, Wahh=Wahh, Wyh=Wyh, bah=bah, by=by,
#                  nonlinearity=nonlinearity, ah0=ah0, LEARN_ah0=False)
# model_name = 'CTRNN'
# numparameters = sum(p.numel() for p in model.parameters() if
#                     p.requires_grad)

model = CTRNN_HL(dim_input, dim_recurrent, dim_output, Wahx=Wahx, Wahh0=Wahh, Wyh=Wyh, bah=bah, by=by,
                 nonlinearity=nonlinearity, ah0=ah0, LEARN_ah0=False, A=A, B=B, C=C, D=D, lrs=lrs)
model_name = 'HL+CTRNN'
numparameters = sum(p.numel() for p in model.parameters() if
                    p.requires_grad)

dir = r'C:\Users\17033\BiologicalSF\HL\saving'# use r'path with spaces' if there are spaces in the path name
fig_dir = fr'C:\Users\\17033\BiologicalSF\HL\saving\{model_name}_{nonlinearity}_dimrecurrent{dim_recurrent}_{numparameters}parameters_{numparameterupdates}parameterupdates{initname}_NOISEAMPLITUDE{NOISEAMPLITUDE}_rng{randseed}'
model_path = fig_dir + f"\\FinalHL+CTRNN.pth"
print(f"Started Saving Tuning Data for Memory {model_name}")
# saveTuningData(model_path, model_name, fig_dir, dim_input, dim_recurrent, dim_output, numT, numtrials, dt, Tau, bhneverlearn, NOISEAMPLITUDE)
numparameters = sum(p.numel() for p in model.parameters() if  p.requires_grad)

# saveTuningData(model_path, model_name, fig_dir, dim_input, dim_recurrent, dim_output, numT, numtrials, dt, Tau, bhneverlearn, NOISEAMPLITUDE)
tuning_path = "HL+CTRNNtuningData.npy"

fig, ax = plt.subplots(10, 10)

col = 0
row = 0

import matplotlib.ticker as ticker
from tqdm import tqdm
import os

os.chdir(fig_dir)
tuningData = np.load(tuning_path)
for i in tqdm(range(100)):
    ax[row][col].imshow((tuningData[:, :, i]))
    ax[row][col].axes.xaxis.set_major_locator(ticker.NullLocator())
    ax[row][col].axes.yaxis.set_major_locator(ticker.NullLocator())

    if (col + 1) % 10 == 0:
        col = 0
    if (i + 1) % 10 == 0:
        row += 1
    else:
        col += 1

fig.suptitle(f'{model_name} Orientation Shown (0 to 360)')
plt.savefig(f"{model_name} memoryTuningCurves.pdf")

getConnectivityFigures(model_path, tuning_path, model_name, fig_dir, dim_input, dim_recurrent, dim_output, numT, numtrials, dt, Tau, bhneverlearn, NOISEAMPLITUDE)
