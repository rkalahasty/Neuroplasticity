def clear_all():
    gl = globals().copy()
    for var in gl:
        if var[0] == '_': continue
        if 'func' in str(globals()[var]): continue
        if 'module' in str(globals()[var]): continue

        del globals()[var]


if __name__ == "__main__":
    clear_all()
# %%-----------------------------------------------------------------------------

import os
print(os.getcwd())
dir = r'C:\Users\17033\BiologicalSF\HL\saving'# use r'path with spaces' if there are spaces in the path name
os.chdir(dir)# print(f'current working direction is {os.getcwd()}')
print(os.getcwd())


import numpy as np
import torch
import matplotlib.pyplot as plt
from matplotlib import cm

from HLmodel import CTRNN, CTRNN_HL, CTRNN_HL_Analysis # from file import function
from generateINandTARGETOUT import generateINandTARGETOUT  # from file import function
torch.autograd.set_detect_anomaly(True)

randseed = 123
np.random.seed(randseed);
torch.manual_seed(randseed)  # set random seed for reproducible results

##############################################################################
# %% initialize network
dim_input = 2  # number of inputs, the first input dimension is for the values to be remembered, the second input dimension is for the go-cue
dim_recurrent = 100  # number of recurrent units
dim_output = 4  # number of outputs, the first output dimension is for the value inputted first, the second output dimension is for the value inputted second
numT = 150  # number of timesteps in a trial
numtrials = 50  # number of trials used for each parameter update, i.e. the number of trials generated by the function generateINandTARGETOUT for each minibatch
nonlinearity = 'retanh'
numparameterupdates = 10000  # number of parameter updates
pset_saveparameters = np.unique(np.concatenate((np.arange(0, 6), np.array([50, 100, 150, 200]), np.round(
    np.linspace(0, numparameterupdates, num=20, endpoint=True))))).astype(
    int)  # save parameters when parameter update p is a member of pset_saveparameters, save as int so we can use elements to load model parameters: for example, model_parameterupdate211.pth versus model_parameterupdate211.0.pth

# model = ElmanRNN(dim_input, dim_recurrent, dim_output, LEARN_h0=False); modelname = 'Elman RNN'
# ------------------------------------------------------
# constants that are not learned: dt, Tau, bhneverlearn
dt = 1
Tau = 10 * torch.ones(dim_recurrent)
NOISEAMPLITUDE = 0.1  # 0.1, standard deviation of firing rate noise, bhneverlearn = NOISEAMPLITUDE*torch.randn(numtrials, numT, dim_recurrent)
bhneverlearn = NOISEAMPLITUDE * torch.randn(numtrials, numT, dim_recurrent)  # (numtrials, numT, dim_recurrent) tensor

# parameters to be learned: Wahh, Wahx, Wyh, bah, by, ah0(optional)
ah0 = torch.zeros(dim_recurrent)
bah = torch.zeros(dim_recurrent)
by = torch.zeros(dim_output)

# Saxe at al. 2014 "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks"
# We empirically show that if we choose the initial weights in each layer to be a random orthogonal matrix (satisifying W'*W = I), instead of a scaled random Gaussian matrix, then this orthogonal random initialization condition yields depth independent learning times just like greedy layerwise pre-training.
# [u,s,v] = svd(A); A = u*s*v’; columns of u are eigenvectors of covariance matrix A*A’; rows of v’ are eigenvectors of covariance matrix A’*A; s is a diagonal matrix that has elements = sqrt(eigenvalues of A’*A and A*A’)
Wahh = np.random.randn(dim_recurrent, dim_recurrent)
u, s, vT = np.linalg.svd(Wahh)  # np.linalg.svd returns v transpose!
Wahh_check = u @ np.diag(s) @ vT
C1 = Wahh;
C2 = Wahh_check
print(
    f"Do C1 and C2 have the same shape and are element-wise equal within a tolerance? {C1.shape == C2.shape and np.allclose(C1, C2)}")
Wahh = u @ np.diag(1.0 * np.ones(dim_recurrent)) @ vT  # make the eigenvalues large so they decay slowly
Wahh = torch.tensor(Wahh, dtype=torch.float32);
initname = '_initWahhsaxe'

# Sussillo et al. 2015 "A neural network that finds a naturalistic solution for the production of muscle activity"
Wahx = torch.randn(dim_recurrent, dim_input) / np.sqrt(dim_input)
Wahh = 1.5 * torch.randn(dim_recurrent, dim_recurrent) / np.sqrt(dim_recurrent)
A = .5 * torch.randn(dim_recurrent, dim_recurrent) / np.sqrt(dim_recurrent)
B = .5 * torch.randn(dim_recurrent, dim_recurrent) / np.sqrt(dim_recurrent)
C = .5 * torch.randn(dim_recurrent, dim_recurrent) / np.sqrt(dim_recurrent)
D = .5 * torch.randn(dim_recurrent, dim_recurrent) / np.sqrt(dim_recurrent)
lrs = .01 * torch.randn(dim_recurrent, dim_recurrent) / np.sqrt(dim_recurrent)

initname = '_initWahhsussillo'
Wyh = torch.zeros(dim_output, dim_recurrent)


#CHANGE WHAT MODEL YOU ARE USING HERE
model1 = CTRNN(dim_input, dim_recurrent, dim_output, Wahx=Wahx, Wahh=Wahh, Wyh=Wyh, bah=bah, by=by,
                 nonlinearity=nonlinearity, ah0=ah0, LEARN_ah0=False)
modelname1 = 'CTRNN'

model2 = CTRNN_HL(dim_input, dim_recurrent, dim_output, Wahx=Wahx.clone(), Wahh0=Wahh.clone(), Wyh=Wyh.clone(), bah=bah.clone(), by=by.clone(),
                 nonlinearity=nonlinearity, ah0=ah0.clone(), LEARN_ah0=False, A=A, B=B, C=C, D=D, lrs=lrs)
modelname2 = 'HL+CTRNN+Adaptation'

model3 = CTRNN_HL_Analysis(dim_input, dim_recurrent, dim_output, Wahx=Wahx.clone(), Wahh0=Wahh.clone(), Wyh=Wyh.clone(), bah=bah.clone(), by=by.clone(),
                 nonlinearity=nonlinearity, ah0=ah0.clone(), LEARN_ah0=False, A=A, B=B, C=C, D=D, lrs=lrs)
modelname3 = 'HL+CTRNN_Analysis'

# # ------------------------------------------------------

for model, modelname in zip([model2], [modelname2]):
    # ---------------check number of learned parameters---------------
    numparameters = sum(p.numel() for p in model.parameters() if
                        p.requires_grad)  # model.parameters include those defined in __init__ even if they are not used in forward pass

    # ---------------make folder to store files---------------
    import os

    figdir = fr'C:\Users\\17033\BiologicalSF\HL\saving\{modelname}_{nonlinearity}_dimrecurrent{dim_recurrent}_{numparameters}parameters_{numparameterupdates}parameterupdates{initname}_NOISEAMPLITUDE{NOISEAMPLITUDE}_rng{randseed}'
    print(os.curdir)
    # if not os.path.exists(figdir):
    os.makedirs(figdir)

    # ---------------save pset_saveparameters---------------
    np.save(f'{figdir}/pset_saveparameters.npy',
            pset_saveparameters)  # pset_saveparameters = np.load(f'{figdir}/pset_saveparameters.npy')

    # ---------------save entire model, not just model parameters---------------
    torch.save(model,
               f'{figdir}/model.pth')  # model = torch.load(f'{figdir}/model.pth')# save entire model, not just model parameters
    # import sys; sys.exit()# stop script at current line


    # ---------------plot initial eigenvalue spectrum of recurrent weight matrix---------------
    plt.figure()  # initial eigenvalue spectrum of recurrent weight matrix

    if modelname == "CTRNN":
        W = model.fc_h2ah.weight.detach().numpy()
    else:
        W = model.Wahh0.detach().numpy()

    eigVal = np.linalg.eigvals(W)
    plt.plot(eigVal.real, eigVal.imag, 'k.', markersize=10)
    plt.xlabel('real(eig(W))')
    plt.ylabel('imag(eig(W))')
    plt.title(f'{modelname}\nInitial eigenvalue spectrum of recurrent weight matrix')
    plt.axis('equal')  # plt.axis('scaled')
    plt.savefig('%s/eigenvaluesW_beforelearning_%s.pdf' % (figdir, modelname.replace(" ", "")),
                bbox_inches='tight')  # add bbox_inches='tight' to keep title from being cutoff
    # import sys; sys.exit()# stop script at current line

    #
    ##############################################################################
    # %% train network
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  # lr = 1e-3 default
    error_store = -700 * np.ones(
        numparameterupdates + 1)  # error_store[0] is the error before any parameter updates have been made, error_store[j] is the error after j parameter updates
    gradient_norm = -700 * np.ones(
        numparameterupdates + 1)  # gradient_norm[0] is norm of the gradient before any parameter updates have been made, gradient_norm[j] is the norm of the gradient after j parameter updates
    figuresuffix = ''

    for p in range(numparameterupdates + 1):  # 0, 1, 2, ... numparameterupdates
        IN, TARGETOUT, itimeRNN = generateINandTARGETOUT(dim_input, dim_output, numT, numtrials, 15, 345)

        bhneverlearn = NOISEAMPLITUDE * torch.randn(numtrials, numT,
                                                        dim_recurrent)  # (numtrials, numT, dim_recurrent) tensor

        output, h, Wahh = model(IN, dt, Tau, bhneverlearn)
        # error = torch.sum((output[itimeRNN == 1] - TARGETOUT[itimeRNN == 1]) ** 2) / torch.sum(
        #     itimeRNN == 1)  # itimeRNN: numtrials x numT x dim_output tensor, elements 0(timepoint does not contribute to this term in the error function), 1(timepoint contributes to this term in the error function)
        # error = torch.sum((output[:, 30:, :][itimeRNN[:, 30:, :] == 1] - TARGETOUT[:, 30:, :][
        #     itimeRNN[:, 30:, :] == 1]) ** 2) / torch.sum(itimeRNN[:, 30:,:] == 1)  # itimeRNN: numtrials x numT x dim_output tensor, elements 0(timepoint does not contribute to this term in the error function), 1(timepoint contributes to this term in the error function)

        output = output
        error = torch.sum((output[itimeRNN == 1] - TARGETOUT[itimeRNN == 1]) ** 2) / torch.sum(itimeRNN == 1)

        error_store[p] = error.item()
        optimizer.zero_grad()
        error.backward(retain_graph=True)
        optimizer.step()
        gradient = []  # store all gradients
        for param in model.parameters():  # model.parameters include those defined in __init__ even if they are not used in forward pass
            if param.requires_grad is True:  # model.parameters include those defined in __init__ even if param.requires_grad is False (in this case param.grad is None)
                gradient.append(param.grad.cpu().detach().flatten().numpy())
        gradient = np.concatenate(gradient)  # gradient = torch.cat(gradient)
        # assert np.allclose(gradient.size,
        #                    model.numparameters), "size of gradient and number of learned parameters don't match!"
        gradient_norm[p] = np.sqrt(np.sum(gradient ** 2))

        if np.isin(p, pset_saveparameters):
            print(f'{p} parameter updates: error = {error.item():.4g}')
            torch.save({'model_state_dict': model.state_dict(), 'figuresuffix': figuresuffix},
                       figdir + f'/model_parameterupdate{p}.pth')  # save the trained model’s learned parameters

    torch.save(model, f'{figdir}/Final{modelname}.pth')

    if modelname == "CTRNN_HL":
        np.save(figdir + f"connectivity{modelname}.npy", Wahh.detach().numpy())
    else:
        np.save(figdir + f"connectivity{modelname}.npy", model.fc_x2ah.weight.detach().numpy())

    # import sys; sys.exit()# stop script at current line
    ##############################################################################
    # %%
    # normalized error, if RNN output is constant for each dim_output (each dim_output can be a different constant) then normalizederror = 100%
    # outputforerror = output(itimeRNN==1)
    # TARGETOUTforerror = TARGETOUT(itimeRNN==1)
    # normalizederror = 100*((outputforerror(:) - TARGETOUTforerror(:))' @ (outputforerror(:) - TARGETOUTforerror(:))) / ((mean(TARGETOUTforerror(:)) - TARGETOUTforerror(:))'@(mean(TARGETOUTforerror(:)) - TARGETOUTforerror(:)));% normalized error when using outputs for which itimeRNNtest = 1
    normalizederror = 0;
    for i in range(dim_output):
        outputforerror = output[:, :, i]  # (numtrialstest, numTtest)
        outputforerror = outputforerror[itimeRNN[:, :, i] == 1][:, None]  # (something,1)
        TARGETOUTforerror = TARGETOUT[:, :, i]  # (numtrialstest, numTtest)
        TARGETOUTforerror = TARGETOUTforerror[itimeRNN[:, :, i] == 1][:, None]  # (something,1)
        A = outputforerror - TARGETOUTforerror
        # B = np.mean(TARGETOUTforerror) - TARGETOUTforerror
        # normalizederror = normalizederror + 100*(A.transpose() @ A) / (B.transpose() @ B)# normalized error when using outputs for which itimeRNN = 1
        B = torch.mean(TARGETOUTforerror) - TARGETOUTforerror
        normalizederror = normalizederror + 100 * (A.transpose(0, 1) @ A) / (
                    B.transpose(0, 1) @ B)  # normalized error when using outputs for which itimeRNN = 1
    normalizederror = torch.squeeze(
        normalizederror) / dim_output  # use torch.squeeze so normalized error goes from having shape (1,1) to being a number, this makes it easier to plot in figure titles, etc.
    # normalizederror = np.squeeze(normalizederror) / dim_output# use np.squeeze so normalized error goes from having shape (1,1) to being a number, this makes it easier to plot in figure titles, etc.

    plt.figure()  # norm of the gradient vs number of parameter updates
    plt.plot(np.arange(0, numparameterupdates + 1), gradient_norm, 'k-', label=modelname)
    plt.xlabel('Number of parameter updates')
    plt.ylabel('Gradient norm during training')
    plt.legend()
    plt.title(
        f"{modelname}\n{numparameterupdates} parameter updates, error = {error_store[-1]:.4g}, normalized error = {normalizederror:.4g}%\nmax = {np.max(gradient_norm):.2g}, min = {np.min(gradient_norm):.2g}, median = {np.median(gradient_norm):.2g}")
    plt.xlim(left=0)
    plt.ylim(bottom=0)
    plt.savefig('%s/gradient_norm%s.pdf' % (figdir, figuresuffix), bbox_inches='tight')

    plt.figure()  # training error vs number of parameter updates
    plt.plot(np.arange(0, numparameterupdates + 1), error_store, 'k-', linewidth=1,
             label=f'{modelname} {error_store[numparameterupdates]:.4g}')
    plt.xlabel('Number of parameter updates')
    plt.ylabel('Mean squared error during training')
    plt.legend()
    plt.title(
        '%s\n%.4g parameter updates, error = %.4g, normalized error = %.4g%%\nerror i%g = %.4g, i%g = %.4g, i%g = %.4g' \
        % (modelname, numparameterupdates, error_store[-1], normalizederror, 5, error_store[5],
           round(numparameterupdates / 2), error_store[round(numparameterupdates / 2)], numparameterupdates,
           error_store[numparameterupdates]))
    plt.xlim(left=0)
    plt.ylim(bottom=0)
    plt.savefig('%s/error_trainingerrorVSparameterupdates%s.pdf' % (figdir, figuresuffix),
                bbox_inches='tight')  # add bbox_inches='tight' to keep title from being cutoff

    plt.figure()  # training error vs number of parameter updates, semilogy
    plt.semilogy(np.arange(0, numparameterupdates + 1), error_store, 'k-', linewidth=1,
                 label=f'{modelname} {error_store[numparameterupdates]:.4g}')
    plt.xlabel('Number of parameter updates')
    plt.ylabel('Mean squared error during training')
    plt.legend()
    plt.title(
        '%s\n%.4g parameter updates, error = %.4g, normalized error = %.4g%%\nerror i%g = %.4g, i%g = %.4g, i%g = %.4g' \
        % (modelname, numparameterupdates, error_store[-1], normalizederror, 5, error_store[5],
           round(numparameterupdates / 2), error_store[round(numparameterupdates / 2)], numparameterupdates,
           error_store[numparameterupdates]))
    plt.xlim(left=0);
    plt.savefig('%s/error_trainingerrorVSparameterupdates_semilogy%s.pdf' % (figdir, figuresuffix),
                bbox_inches='tight')  # add bbox_inches='tight' to keep title from being cutoff

    # plt.figure()  # final eigenvalue spectrum of recurrent weight matrix
    # if modelname == 'HL+CTRNN': W = model.Wahh.detach().numpy();
    # if modelname == 'CTRNN': W = model.fc_h2ah.weight.detach().numpy();
    # eigVal = np.linalg.eigvals(W)
    # plt.plot(eigVal.real, eigVal.imag, 'k.', markersize=10)
    # plt.xlabel('real(eig(W))')
    # plt.ylabel('imag(eig(W))')
    # plt.title(
    #     f'{modelname}\neigenvalue spectrum of recurrent weight matrix after {numparameterupdates} parameter updates')
    # plt.axis('equal')  # plt.axis('scaled')
    # plt.savefig('%s/eigenvaluesW_%gparameterupdates_%s%s.pdf' % (
    # figdir, numparameterupdates, modelname.replace(" ", ""), figuresuffix),
    #             bbox_inches='tight')  # add bbox_inches='tight' to keep title from being cutoff
    # # import sys; sys.exit()# stop script at current line
